\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{PDE: Smoothing}
\date{June 2020}

\begin{document}

\maketitle

\section{Introduction}
 
It is universally acknowledged that observed data in the physical world is, to some extent, contaminated with random noise. Hence, one can express our data generating mechanism using two components: a true underlying oracle function u(x) and a random component epsilon. It arises naturally that one would like to know u(x) as it is the data generator's essence. Yet, due to the fundamental sorrow -the limited observations of an infinite world, we fail to achieve the oracle function. Instead, we can only estimate it based on our finite noise infested observations. 

For decades, statisticians have been proposed various methods to estimate the oracle function u(x). One of the non-parametric ways is to use direct diffusion to smooth away all the noise. In an absolute non-rigorous sense, the direction diffusion process achieves the oracle risk by (weighted) averaging each observation neighbor. Each time we do this, our estimated function will be closer to the true function. Until, at a finite time T, we will achieve our goal. After that, at time T + 1, the estimated function will again be further away. People coin the term "over-smoothing" to indicate this phenomenon. Our goal is to discover the optimal T where we are closest to the truth given an observed data set. 

However, this seemingly simple task proves to be somewhat challenging. This is because the diffusion process is not widely used among statisticians. Luckily, it is known that the famous kernel (Gaussian) smoother and the direct diffusion (heat equation) are equivalents. Their connection is expressed through the equation T = 0.5 $\sigma$. T denotes the smoothing time in the direction process. And $\sigma$ denotes the spatial width of the Gaussian smoother. Therefore, finding optimal time T can be translated to the problem of finding the optimal sigma. In statistics literature, the optimal sigma is found to be $(1/2N\pi||f''||^2)^{2/5}$ a constant for every point x. This is a significant constraint. Hence, in this paper, we will allow the spatial bandwidth to be different for each point. In the next section, we will derive the optimal sigma for each observation point. We will do simulations to show that our "adaptive" bandwidth performs better than the best-fixed bandwidth. 

\section{One way of thinking}

First order Taylor expansion: 
\begin{align}
  u(x_i) \approx u(x) + \frac{\partial u}{\partial x}(x_i - x) 
\end{align}
Then 
\begin{align}
   y_i &\approx  u(x) + \frac{\partial u}{\partial x}(x_i - x) + \epsilon_i \\ 
    \hat{u}(x) &= \sum_{i=1}^n y_i w_i \\
   &= \sum_{i=1}^n \left( u(x) + \frac{\partial u}{\partial x}(x_i - x) + \epsilon_i \right) w_i \\
   &= u(x) + \sum_{i=1}^n  w_i \epsilon_i +  \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  w_i(x_i - x) \\ 
   \hat{u}(x) - u(x) &= \sum_{i=1}^n  w_i \epsilon_i +  \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  w_i(x_i - x) \\ 
     E\left[ \left(\hat{u}(x) - u(x)\right)^2 \right] &=  E\left[ \left(w_1\epsilon_1+w_2\epsilon_2+...+w_n\epsilon_n \right)^2 \right] \\&+  E\left[ \left( \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  w_i(x_i - x)\right)^2 \right] \\ 
     E\left[ \left(\hat{u}(x_i) - u(x_i)\right)^2 \right] &=  \sigma^2\sum_{i=1}^n  w_i^2  +  \left[ \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  w_i(x_i - x) \right]^2 
\end{align}


\subsection{Linear}
We let

\begin{align}
    \phi_b (x) = \frac{1}{\sqrt{2 \pi b}}\exp(\frac{-x^2}{2b})
\end{align}

where b is the band width parameter. Then we have our MSE at $x_i$

\begin{align}
   MSE_j =  \sigma^2\sum_{i=1}^n  \phi_b^2(x_j-y_i)  +  \left[ \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  \phi_b(x_j-y_i)\cdot(x_j - y_i) \right]^2 
\end{align}

Now we replace summation with integration

\begin{align}
     MSE_j =  \sigma^2 \int_\omega  \phi_b^2(x_i-y)dy  +  \left[ \frac{\partial u}{\partial x}\cdot \int_\omega  \phi_b(x_j-y_i)\cdot(x_j - y)dy \right]^2 \\
     = \sigma^2 \sqrt{2\pi \cdot b}
\end{align}


\subsection{Quadratic}

We let

\begin{align}
    \phi_b (x) = \frac{1}{\sqrt{2 \pi b}}\exp(\frac{-x^2}{2b})
\end{align}

where b is the band width parameter. Then we have our MSE at $x_i$


\begin{align}
        \left(\hat{u}(x_i) - u(x_i)\right)  &=  \sigma^2\sum_{i=1}^n  w_i^2  + \left[ \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  w_i(x_i - x) \right] +  \left[ \frac{\partial^2 u}{2\partial x^2}\cdot \sum_{i=1}^n  w_i(x_i - x)^2 \right] + \sum_{i=1}^n \epsilon_i w_i \\
        E\left[ \left(\hat{u}(x_i) - u(x_i)\right)^2 \right] &= Var\left[ \left(\hat{u}(x_i) - u(x_i)\right) \right] + (E\left[ \left(\hat{u}(x_i) - u(x_i)\right) \right])^2 \\ &=\sigma^2\sum_{i=1}^n  w_i^2  + \left[\frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  w_i(x_i - x)  +   \frac{\partial^2 u}{2\partial x^2}\cdot \sum_{i=1}^n  w_i(x_i - x)^2 \right]^2 \\
   MSE_j =  \sigma^2\sum_{i=1}^n  \phi_b^2(x_j-y_i) & +  \left[ \frac{\partial u}{\partial x}\cdot \sum_{i=1}^n  \phi_b(x_j-y_i)\cdot(x_j - y_i) + \frac{\partial^2 u}{2\partial x^2}\cdot \sum_{i=1}^n  \phi_b(x_j-y_i)\cdot(x_j - y_i)^2 \right]^2
\end{align}

Now we replace summation with integration

\begin{align}
     MSE_j &=  \sigma^2 \int_\omega  \phi_b^2(x_i-y)dy  +  \left[ \frac{\partial u}{\partial x}\cdot \int_\omega  \phi_b(x_j-y)\cdot(x_j - y)dy + \frac{\partial^2 u}{2\partial x^2}\cdot \int_{\omega}  \phi_b(x_j-y)\cdot(x_j - y)^2 dy\right]^2\\
     &= \frac{\sigma^2}{2\sqrt{\pi \cdot b}}   + 0 + \left[ \frac{\partial^2 u}{2\partial x^2}\cdot \int_{\omega}  \phi_b(x_j-y)\cdot(x^2_j -2x_j\cdot y + y^2) dy\right]^2\\
     &=  \frac{\sigma^2}{2\sqrt{\pi \cdot b}}   + \left[ \frac{\partial^2 u}{2\partial x^2}\cdot \int_{\omega}  \phi_b(x_j-y)(x^2_j)dy - 2 \int_{\omega}  \phi_b(x_j-y)\cdot (x_j\cdot y)dy +  \int_{\omega}  \phi_b(x_j-y)\cdot y^2 dy\right]^2\\
     &=  \frac{\sigma^2}{2\sqrt{\pi \cdot b}}   + \left[\frac{\partial^2 u}{2\partial x^2}\cdot (x^2_j - 2x^2_j + b + x^2_j) \right]^2\\
     &=  \frac{\sigma^2}{2\sqrt{\pi \cdot b}}  + \left[\frac{\partial^2 u}{2\partial x^2}\cdot b \right]^2
\end{align}



Set $h(b) = \frac{\sigma^2}{2\sqrt{\pi}}\cdot \frac{1}{\sqrt{b}}+   \frac{1}{4}   u''^2\cdot b^2$, and take derivative, then set $h'(b) = 0$
\begin{align}
    h'(b)&= -\frac{\sigma^2}{4\sqrt{\pi}}\cdot b^{-\frac{3}{2}}+\frac{1}{2} u''^\cdot b = 0\\
    b &=\left(\frac{\sigma^2}{2\sqrt{\pi} u''^2}\right)^{\frac{2}{5}} 
\end{align}











\section{Another way of thinking}

Gaussian Kernel: 

$$w_{ij}(x_j, g)  = \exp[- \frac{(x_i - x_j)^2}{2g^2}]$$

We have 

$$MSE(x_j, g) = E[\hat{\mu}(x_j) - \mu(x_j)]^2$$

We will need to minimize 

$$MSE(g) = \sum_{j=1}^n MSE(x_j, g)$$

We have 

$$\hat{\mu}(x_j, i) = \mu(x_j) + \sum_{i = 1}^{n} w_{ij}(\mu'(x_j)(x_i-x_j) + \epsilon_i)  $$

Where $\epsilon_i$ follows $N(0, \sigma^2)$

Hence 

$$MSE(x_j,g) = E[\sum_{i = 1}^{n} w_{ij}(\mu'(x_j)(x_i-x_j) + \epsilon_i) ]^2$$
$$=Var(\sum_{i = 1}^{n} w_{ij}(\mu'(x_j)(x_i-x_j) + \epsilon_i)) + (E[\sum_{i = 1}^{n} w_{ij}(\mu'(x_j)(x_i-x_j) + \epsilon_i) ])^2$$
$$=\sigma^2\sum_{i=1}^n w_{ij}^2 + [\sum_{i=1}^n w_{ij}\mu'(x_j)(x_i-x_j)]^2$$



\section{Question}



\begin{align*}
    Y_i &= g(x_i)+\epsilon_i\\ 
    MSE &= \sum_{i=1}^N(\hat{Y}_i- Y_i)^2 = \sum_{i=1}^N(\hat{Y}_i- g(x_i))^2\\ 
    MISE &= \int_1^5[\hat{g}(x) - g(x)]^2dx =\int_1^5[\hat{Y}_i - g(x)]^2dx =  \sum_{i=1}^N[\hat{Y}_i - g(x)]^2\cdot dx
\end{align*}












\end{document}







\end{document}
